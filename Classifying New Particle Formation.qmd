---
title: "Classifying New Particle Formation"
execute: 
  echo: false
format:
  pdf
---
```{python}
#Tähän ulkopuoliset importit
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
```

```{python}
#Tähän Eliaksen importit

```

```{python}
#Tähän Joken importit
import pca
```

Group: John 117

Members: Elias Toukolehto and Joacim Sarén

# Methodology

We decided to start by trying feature selection and dimensionality reduction via principal component analysis with a few different machine learning models.

# Principal component analysis

First we looked at cumulative proportion of variance explained by the principal components. We decided to use PCA for it's relavive simplicity. We used l2 normalization in order to keep all predictors in consideration when running PCA. We looked at the behaviour of PCA and some the models at up to 100 components, but settled on a maximum of 40 for comparisons. The models we did test with more diddn't show interesting behaviour beyond 40 and training random forest takes a long time with many components.

```{python}
plt.plot(range(1, 41), pca.cum_pve)
plt.hlines(1, 0, 40, 'red', 'dotted')
plt.title('Cumulative portion of variance explained')
plt.show()
```

Then we looked at the accuracy of logistic regression, decision tree and gaussian naive Bayes classifiers with different numbers of principal components for binary and 4-class classification using 10-fold cross-validation.

```{python}
plt.plot(range(1, 41), pca.nbscore2, color='red', label='Naive Bayes')
plt.plot(range(1, 41), pca.treescore2, color='blue', label='Decision tree')
plt.plot(range(1, 41), pca.lrscore2, color='green', label='logistic regression')
plt.legend()
plt.title('Binary classification accuracy')
plt.grid(True, 'both')
plt.show()

plt.plot(range(1, 41), pca.nbscore4, color='red', label='Naive Bayes')
plt.plot(range(1, 41), pca.treescore4, color='blue', label='Decision tree')
plt.plot(range(1, 41), pca.lrscore4, color='green', label='logistic regression')
plt.legend()
plt.title('4-class classification accuracy')
plt.grid(True, 'both')
plt.show()
```

Logistic regression achieves the highest accuracy in both binary and 4-class classification. Because the task focuses on binary classification, which is also easier than multiclass classification, we wanted to explore a 2-tier classifier. First tier seperates events and non-events, and the 2nd classifies events to the specific event classes. To choose the model for the 2nd tier, we looked at the 3-class classification accuracy of thea same classification methods trained on data only containing events.

The data with 10-fold cross-validation was inconclusive as accuracy for logistig regression jumped up and down in around the same range with 15 or less components. We suspect the main reason to be the small population of the smallest class. Out of the 225 events in the training data, only 26 are in class Ia. To smooth out the result, we decided to use 26-fold CV to match this population size. Our cross validation method maintains class sizes within folds, so the validation set always includes one Ia event. 

```{python}
plt.plot(range(1, 41), pca.nbscore3, color='red', label='Naive Bayes')
plt.plot(range(1, 41), pca.treescore3, color='blue', label='Decision tree')
plt.plot(range(1, 41), pca.lrscore3, color='green', label='logistic regression')
plt.legend()
plt.title('3-class classification accuracy')
plt.grid(True, 'both')
plt.show()
```

The results still vary at higher component counts, but we can pick some welll performing models from the lower component count ones Random forest with 37 components has the highest accuracy, but the variance in this component count range is high. Clear improvement stops at 13 components. Logistic regression reaches its peak accuracy at 22 components, but is almost as accurate with just 9 components. Naive Bayes with 16 components is between them, and clearly the best result for NB.

# PCA-based 2-tier classifier

Based on results shown in the previous chapter, we used logistic regression with 12 principal components to seperate events and non-events. For event classification we tested LR, RF and NB with 9, 13 and 16 components respectively.

# Feature selection

Another preprocessing method is feature selection, for which we have used Recursive Feature elimination using 5-fold cross-validation, which is a form of backward selection.
With RFE we get the following results for binary classification

```{python}
RFE_df_binary = pd.read_csv('RFE_binary.csv')
RFE_df_multi = pd.read_csv('RFE_multi.csv')
print(RFE_df_binary)
print(RFE_df_multi)
```

on this table we can compare the improvements RFE provides, able to increase binary classification accuracy by couple percent compared to including all features.

```{python}
cv_results_binary = np.load('binaryCV.npy')

LR1_results = pd.DataFrame(cv_results_binary[0][:40])
LR2_results = pd.DataFrame(cv_results_binary[1][:40])
DT_results = pd.DataFrame(cv_results_binary[2][:40])
RF_results = pd.DataFrame(cv_results_binary[3][:40])
SVC_results = pd.DataFrame(cv_results_binary[4][:40])

plt.plot(LR1_results[2], LR1_results[0], color='red', label='LR (Lasso)')
plt.plot(LR2_results[2], LR2_results[0], color='blue', label='LR (Ridge)')
plt.plot(DT_results[2], DT_results[0], color='green', label='Decicion Tree')
plt.plot(RF_results[2], RF_results[0], color='orange', label='Random Forest')
plt.plot(SVC_results[2], SVC_results[0], color='purple', label='Support Vector Classification')

plt.legend()
plt.title('RFE binary classification accuracy')

plt.show() 

```

We can see that increasing features results in better accuracy, usually having the best accuracy at 10-20 features.
We currently only have linear regression with L1 (lasso) and L2 (ridge) penalties. After comparing different models, we will probably use some combination of PCA and Feature selection in our final model

```{python}

cv_results_multi = np.load('multiCV.npy')

LR1_results2 = pd.DataFrame(cv_results_multi[0][:40])
LR2_results2 = pd.DataFrame(cv_results_multi[1][:40])
DT_results2 = pd.DataFrame(cv_results_multi[2][:40])
RF_results2 = pd.DataFrame(cv_results_multi[3][:40])
SVC_results2 = pd.DataFrame(cv_results_multi[4][:40])

plt.plot(LR1_results2[2], LR1_results2[0], color='red', label='LR (Lasso)')
plt.plot(LR2_results2[2], LR2_results2[0], color='blue', label='LR (Ridge)')
plt.plot(DT_results2[2], DT_results2[0], color='green', label='Decicion Tree')
plt.plot(RF_results2[2], RF_results2[0], color='orange', label='Random Forest')
plt.plot(SVC_results2[2], SVC_results2[0], color='purple', label='Support Vector Classification')

plt.legend()
plt.title('RFE 3-class accuracy')

plt.show() 
```

# Feature analysis

Analyzing coefficients allowed us to rank some features by usefulness. The ones for the plot below were defined by performance with logistic regression using 5-fold cross-validation.

```{python}
df = pd.read_csv('train.csv')

df = df.drop(columns=['id', 'date', 'partlybad'])

train_x = df.drop(columns=['class4'])
train_x = (train_x-train_x.mean())/train_x.std()

train_y = df['class4']

df = train_x.copy()
df['class4'] = train_y

best5 = ['CS.mean', 'SWS.std', 'RHIRGA672.mean', 'PAR.mean', 'Glob.mean']
worst5 = ['NOx672.mean', 'CO2504.std','NO168.std', 'NO336.mean', 'CO2504.std']

nonevent_df = df[df['class4'] == 'nonevent']
event_df = df[df['class4'] != 'nonevent']
ia_df = df[df['class4'] == 'Ia']
ib_df = df[df['class4'] == 'Ib']
ii_df = df[df['class4'] == 'II']

k = 6
k2 = int(k/2)
fig, axs = plt.subplots(2,k2)
for i in range(0,k):

  if i < k2:
    axs[0,i].set_title(best5[i])
    l = [nonevent_df[best5[i]],event_df[best5[i]], ia_df[best5[i]], ib_df[best5[i]], ii_df[best5[i]]]
    axs[0,i].boxplot(l, tick_labels=['non','any', 'Ia', 'Ib', 'II'], sym='_')
  else:
    j = int(i-k2)
    axs[1,j].set_title(worst5[j])
    l = [nonevent_df[worst5[j]],event_df[worst5[j]], ia_df[worst5[j]], ib_df[worst5[j]], ii_df[worst5[j]]]
    axs[1,j].boxplot(l, tick_labels=['non','any', 'Ia', 'Ib', 'II'], sym='_')

plt.tight_layout()
plt.show()
```

Here are boxplots of some of the most useful (top row), and least useful (bottom row) features for binary classification in the dataset. For the reliable features we can see that the quartaile ranges don't have much overlap, as for the unreliable features there is a lot of overlap between classes. CS.mean is an oulkier, where models find it very useful even though it has surprisingly similar boxplots between classes.