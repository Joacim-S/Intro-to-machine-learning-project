---
title: "Classifying New Particle Formation"
execute: 
  echo: false
format:
  pdf
---
```{python}
#Tähän ulkopuoliset importit
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
```

```{python}
#Tähän Eliaksen importit

```

```{python}
#Tähän Joken importit
import pca
```

Group: John 117

Members: Elias Toukolehto and Joacim Sarén

# Methodology

We were interested in comparing the effects principal component analysis and feature selection have on different machine learning models. We decided to rely on cross-validation for estimating accuracy. We wanted to start simple and increase the complexity of our models over time. We began with logistic regression models, and realized that we should not use L1 or L2 penalties with PCA, but decided to use them for feature selection because the performance was better. Once we had logistic regression implemented for both methologies, we moved on to other models.

Our method for feature selection was Recursive Feature elimination, which required models to rank features in one way or another. Therefore we decided not to implement models that do not  have such ranking features in feature selection. Naive Bayes classifier is an example of such model. However, since the principal components are the same for each model, we had no such issues combining naive Bayes with PCA.

At this point we had some models, but we were unhappy with their performance. We decided to split our model into two tiers. The first tier would only do binary classification, while the second tier would only classify between event types. This way if we found a modelthat performs better for either tier, we can combine them in our model for optimal results. Finally we tried adding more models we have encountered during the cource, but their performances were qiote low.


# Principal component analysis

First we looked at cumulative proportion of variance explained by the principal components. We decided to use PCA for it's relavive simplicity. We used l2 normalization in order to keep all predictors in consideration when running PCA. We looked at the behaviour of PCA and some the models at up to 100 components, but settled on a maximum of 40 for comparisons. The models we did test with more diddn't show interesting behaviour beyond 40 and training random forest takes a long time with many components.

```{python}
plt.plot(range(1, 41), pca.cum_pve)
plt.hlines(1, 0, 40, 'red', 'dotted')
plt.title('Cumulative portion of variance explained')
plt.show()
```

Then we looked at the accuracy of logistic regression, decision tree and gaussian naive Bayes classifiers with different numbers of principal components for binary and 4-class classification using 10-fold cross-validation.

```{python}
plt.plot(range(1, 41), pca.nbscore2, color='red', label='Naive Bayes')
plt.plot(range(1, 41), pca.treescore2, color='blue', label='Decision tree')
plt.plot(range(1, 41), pca.lrscore2, color='green', label='logistic regression')
plt.legend()
plt.title('Binary classification accuracy')
plt.grid(True, 'both')
plt.show()

plt.plot(range(1, 41), pca.nbscore4, color='red', label='Naive Bayes')
plt.plot(range(1, 41), pca.treescore4, color='blue', label='Decision tree')
plt.plot(range(1, 41), pca.lrscore4, color='green', label='logistic regression')
plt.legend()
plt.title('4-class classification accuracy')
plt.grid(True, 'both')
plt.show()
```

Logistic regression achieves the highest accuracy in both binary and 4-class classification. Because the task focuses on binary classification, which is also easier than multiclass classification, we wanted to explore a 2-tier classifier. First tier seperates events and non-events, and the 2nd classifies events to the specific event classes. To choose the model for the 2nd tier, we looked at the 3-class classification accuracy of thea same classification methods trained on data only containing events.

The data with 10-fold cross-validation was inconclusive as accuracy for logistig regression jumped up and down in around the same range with 15 or less components. We suspect the main reason to be the small population of the smallest class. Out of the 225 events in the training data, only 26 are in class Ia. To smooth out the result, we decided to use 26-fold CV to match this population size. Our cross validation method maintains class sizes within folds, so the validation set always includes one Ia event. 

```{python}
plt.plot(range(1, 41), pca.nbscore3, color='red', label='Naive Bayes')
plt.plot(range(1, 41), pca.treescore3, color='blue', label='Decision tree')
plt.plot(range(1, 41), pca.lrscore3, color='green', label='logistic regression')
plt.legend()
plt.title('3-class classification accuracy')
plt.grid(True, 'both')
plt.show()
```

The results still vary at higher component counts, but we can pick some welll performing models from the lower component count ones Random forest with 37 components has the highest accuracy, but the variance in this component count range is high. Clear improvement stops at 13 components. Logistic regression reaches its peak accuracy at 22 components, but is almost as accurate with just 9 components. Naive Bayes with 16 components is between them, and clearly the best result for NB.

# PCA-based 2-tier classifier

Based on results shown in the previous chapter, we used logistic regression with 12 principal components to seperate events and non-events. For event classification we tested LR, RF and NB with 9, 13 and 16 components respectively.

# Feature selection

Another preprocessing method is feature selection, for which we have used Recursive Feature elimination using 5-fold cross-validation, which is a form of backward selection.
With RFE we get the following results for binary, and 3-class classification.

```{python}
RFE_df_binary = pd.read_csv('RFE_binary.csv')
RFE_df_multi = pd.read_csv('RFE_multi.csv')
print(RFE_df_binary)
print(RFE_df_multi)
```

Our best model for binary classification ends up being Logistic regression with L1 penalty, using 17 features reaching accuracy of 89.5%. This is a vey slight increase to the accuracy we recieve without feature selection (89.4%).
The increase in model performance is more significant in the 3-class classification, being around 6% for most models. Once again linear regressiion is the best performing model, and with surprisingly few features. 

```{python}
cv_results_binary = np.load('binaryCV.npy')

LR1_results = pd.DataFrame(cv_results_binary[0][:40])
LR2_results = pd.DataFrame(cv_results_binary[1][:40])
DT_results = pd.DataFrame(cv_results_binary[2][:40])
RF_results = pd.DataFrame(cv_results_binary[3][:40])
SVC_results = pd.DataFrame(cv_results_binary[4][:40])

plt.plot(LR1_results[2], LR1_results[0], color='red', label='LR (Lasso)')
plt.plot(LR2_results[2], LR2_results[0], color='blue', label='LR (Ridge)')
plt.plot(DT_results[2], DT_results[0], color='green', label='Decicion Tree')
plt.plot(RF_results[2], RF_results[0], color='orange', label='Random Forest')
plt.plot(SVC_results[2], SVC_results[0], color='purple', label='Support Vector Classification')

plt.legend()
plt.title('RFE binary classification accuracy')

plt.show() 

```

We can see that increasing features results in better accuracy, usually having the best accuracy at 10-20 features. Decicion treehas it's best accuracy at only two features, while random forest has it all the way at 25. The accuracy does not increace or decreace much after 15 features for most models

```{python}

cv_results_multi = np.load('multiCV.npy')

LR1_results2 = pd.DataFrame(cv_results_multi[0][:40])
LR2_results2 = pd.DataFrame(cv_results_multi[1][:40])
DT_results2 = pd.DataFrame(cv_results_multi[2][:40])
RF_results2 = pd.DataFrame(cv_results_multi[3][:40])
SVC_results2 = pd.DataFrame(cv_results_multi[4][:40])

plt.plot(LR1_results2[2], LR1_results2[0], color='red', label='LR (Lasso)')
plt.plot(LR2_results2[2], LR2_results2[0], color='blue', label='LR (Ridge)')
plt.plot(DT_results2[2], DT_results2[0], color='green', label='Decicion Tree')
plt.plot(RF_results2[2], RF_results2[0], color='orange', label='Random Forest')
plt.plot(SVC_results2[2], SVC_results2[0], color='purple', label='Support Vector Classification')

plt.legend()
plt.title('RFE 3-class accuracy')

plt.show() 
```

Looking at the graph for accuracy per number of features for 3-class RFE, we can see that the differences each feature makes are lot more significant, even with 20+ features. The accuracy is quite poor, and for most models it's best with only very few features.


# Feature analysis

Analyzing coefficients allowed us to rank some features by usefulness. The ones for the plot below were defined by performance with logistic regression using 5-fold cross-validation.

```{python}
df = pd.read_csv('train.csv')

df = df.drop(columns=['id', 'date', 'partlybad'])

train_x = df.drop(columns=['class4'])
train_x = (train_x-train_x.mean())/train_x.std()

train_y = df['class4']

df = train_x.copy()
df['class4'] = train_y

best5 = ['CS.mean', 'SWS.std', 'RHIRGA672.mean', 'PAR.mean', 'Glob.mean']
worst5 = ['NOx672.mean', 'CO2504.std','NO168.std', 'NO336.mean', 'CO2504.std']

nonevent_df = df[df['class4'] == 'nonevent']
event_df = df[df['class4'] != 'nonevent']
ia_df = df[df['class4'] == 'Ia']
ib_df = df[df['class4'] == 'Ib']
ii_df = df[df['class4'] == 'II']

k = 6
k2 = int(k/2)
fig, axs = plt.subplots(2,k2)
for i in range(0,k):

  if i < k2:
    axs[0,i].set_title(best5[i])
    l = [nonevent_df[best5[i]],event_df[best5[i]], ia_df[best5[i]], ib_df[best5[i]], ii_df[best5[i]]]
    axs[0,i].boxplot(l, tick_labels=['non','any', 'Ia', 'Ib', 'II'], sym='_')
  else:
    j = int(i-k2)
    axs[1,j].set_title(worst5[j])
    l = [nonevent_df[worst5[j]],event_df[worst5[j]], ia_df[worst5[j]], ib_df[worst5[j]], ii_df[worst5[j]]]
    axs[1,j].boxplot(l, tick_labels=['non','any', 'Ia', 'Ib', 'II'], sym='_')

plt.tight_layout()
plt.show()
```

Here are boxplots of some of the most useful (top row), and least useful (bottom row) features for binary classification in the dataset. For the reliable features we can see that the quartaile ranges don't have much overlap, as for the unreliable features there is a lot of overlap between classes. CS.mean is an outlier, where models find it very useful even though it has surprisingly similar boxplots between classes. Some features that are good with binary classification, are less useful in 3-class classification. RHIRGA672.mean is an example of such feature. This is one of the strengths of using two tiers with our model for feature selection.

# The final submission
We looked trough plenty of different models, but in the end found hat Logistic regression performs best with our data, at least according to cross-validation.

# Conclusions
We considered a few possible improvements to our model, but did not have time to implement them. One obvious idea was to increase our model from 2-tier model into a 3-tier model. This would let us improve between stages once more. The second idea was to add engineered features to the data. The data is the means and variances of different units, and by fitting them into some equation, we could have features that make classifying the data easier. We also did not take prplexity of our models at all into account, and by also measuring perplexity, we could have optimized our submissions further.

We were still able to test plenty of different models, and compare their accuracy scores by adjusting our features. HAving two tiers in our models, allows us to pick the best amount of features for both stages, slightly improving our accuracy

# Grading suggestion

Even though our end results were fairly inaccurate, we used plenty of time testing different approaches in preprocessing our data and testing different models. We think that the end result and amount of effort is enough to have our group project graded 3 out of 5